import pefile
import numpy as np

def extract_features(file_path):
    try:
        pe = pefile.PE(file_path)
        
        # Example of extracting features (should match the features used in training)
        features = [
            pe.FILE_HEADER.Machine,
            pe.FILE_HEADER.SizeOfOptionalHeader,
            pe.FILE_HEADER.Characteristics,
            pe.OPTIONAL_HEADER.MajorLinkerVersion,
            pe.OPTIONAL_HEADER.MinorLinkerVersion,
            pe.OPTIONAL_HEADER.SizeOfCode,
            pe.OPTIONAL_HEADER.SizeOfInitializedData,
            pe.OPTIONAL_HEADER.SizeOfUninitializedData,
            pe.OPTIONAL_HEADER.AddressOfEntryPoint,
            pe.OPTIONAL_HEADER.BaseOfCode,
            pe.OPTIONAL_HEADER.ImageBase,
            pe.OPTIONAL_HEADER.SectionAlignment,
            pe.OPTIONAL_HEADER.FileAlignment,
            pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MajorImageVersion,
            pe.OPTIONAL_HEADER.MinorImageVersion,
            pe.OPTIONAL_HEADER.MajorSubsystemVersion,
            pe.OPTIONAL_HEADER.MinorSubsystemVersion,
            pe.OPTIONAL_HEADER.SizeOfImage,
            pe.OPTIONAL_HEADER.SizeOfHeaders,
            pe.OPTIONAL_HEADER.CheckSum,
            pe.OPTIONAL_HEADER.Subsystem,
            pe.OPTIONAL_HEADER.DllCharacteristics,
            pe.OPTIONAL_HEADER.SizeOfStackReserve,
            pe.OPTIONAL_HEADER.SizeOfStackCommit,
            pe.OPTIONAL_HEADER.SizeOfHeapReserve,
            pe.OPTIONAL_HEADER.SizeOfHeapCommit,
            pe.OPTIONAL_HEADER.LoaderFlags,
            pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,
            len(pe.sections),  # Number of sections
        ]
        
        # Section features
        sections_mean_entropy = np.mean([section.get_entropy() for section in pe.sections])
        sections_min_entropy = np.min([section.get_entropy() for section in pe.sections])
        sections_max_entropy = np.max([section.get_entropy() for section in pe.sections])
        sections_mean_rawsize = np.mean([section.SizeOfRawData for section in pe.sections])
        sections_min_rawsize = np.min([section.SizeOfRawData for section in pe.sections])
        sections_max_rawsize = np.max([section.SizeOfRawData for section in pe.sections])
        sections_mean_virtualsize = np.mean([section.Misc_VirtualSize for section in pe.sections])
        sections_min_virtualsize = np.min([section.Misc_VirtualSize for section in pe.sections])
        sections_max_virtualsize = np.max([section.Misc_VirtualSize for section in pe.sections])
        
        features.extend([
            sections_mean_entropy,
            sections_min_entropy,
            sections_max_entropy,
            sections_mean_rawsize,
            sections_min_rawsize,
            sections_max_rawsize,
            sections_mean_virtualsize,
            sections_min_virtualsize,
            sections_max_virtualsize,
        ])
        
        # Import features
        imports_nb_dll = len(pe.DIRECTORY_ENTRY_IMPORT) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb = sum([len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb_ordinal = sum([len([imp for imp in entry.imports if imp.ordinal]) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        
        features.extend([
            imports_nb_dll,
            imports_nb,
            imports_nb_ordinal,
        ])
        
        # Export features
        export_nb = len(pe.DIRECTORY_ENTRY_EXPORT.symbols) if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') else 0
        features.append(export_nb)
        
        # Resource features
        resources_nb = len(pe.DIRECTORY_ENTRY_RESOURCE.entries) if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0
        resources = []
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(entry, 'directory'):
                    for res in entry.directory.entries:
                        data_rva = res.data.struct.OffsetToData
                        size = res.data.struct.Size
                        data = pe.get_data(data_rva, size)
                        entropy = entropy_calculate(data)
                        resources.append((entropy, size))
        
        if resources:
            resources_mean_entropy = np.mean([res[0] for res in resources])
            resources_min_entropy = np.min([res[0] for res in resources])
            resources_max_entropy = np.max([res[0] for res in resources])
            resources_mean_size = np.mean([res[1] for res in resources])
            resources_min_size = np.min([res[1] for res in resources])
            resources_max_size = np.max([res[1] for res in resources])
        else:
            resources_mean_entropy = 0
            resources_min_entropy = 0
            resources_max_entropy = 0
            resources_mean_size = 0
            resources_min_size = 0
            resources_max_size = 0
        
        features.extend([
            resources_nb,
            resources_mean_entropy,
            resources_min_entropy,
            resources_max_entropy,
            resources_mean_size,
            resources_min_size,
            resources_max_size,
        ])
        
        # Load configuration size
        load_configuration_size = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size if hasattr(pe, 'DIRECTORY_ENTRY_LOAD_CONFIG') else 0
        features.append(load_configuration_size)
        
        # Version information size
        version_information_size = len(pe.VS_FIXEDFILEINFO) if hasattr(pe, 'VS_FIXEDFILEINFO') else 0
        features.append(version_information_size)
        
        # Ensure feature vector size matches model expectation
        expected_feature_size = 53  # Adjust this number based on your model
        if len(features) != expected_feature_size:
            print(f"Warning: Expected {expected_feature_size} features, but got {len(features)} features.")
            # Pad or truncate the feature list
            features = (features + [0] * expected_feature_size)[:expected_feature_size]
        
        return features
    
    except Exception as e:
        print(f"Error extracting features: {e}")
        return [0] * 53  # Adjust this number based on your model

def entropy_calculate(data):
    """Calculate the entropy of a given data."""
    if not data:
        return 0
    entropy = 0
    data_len = len(data)
    freq = {}
    for byte in data:
        if byte in freq:
            freq[byte] += 1
        else:
            freq[byte] = 1
    
    for byte in freq:
        p_x = freq[byte] / data_len
        entropy -= p_x * np.log2(p_x)
    return entropy
