import os
import pickle
import pandas as pd
from django.shortcuts import render
from django.http import JsonResponse
from django.core.files.storage import FileSystemStorage
from sklearn.tree import DecisionTreeClassifier
import pefile
import numpy as np

# Load the trained model
model_path = os.path.join(os.path.dirname(__file__), 'model', 'MalwareDetectionModel.pkl')
with open(model_path, 'rb') as model_file:
    model = pickle.load(model_file)





def extract_features(file_path):
    try:
        pe = pefile.PE(file_path)
        
        # Example of extracting features
        features = [
            pe.FILE_HEADER.Machine,
            pe.FILE_HEADER.SizeOfOptionalHeader,
            pe.FILE_HEADER.Characteristics,
            pe.OPTIONAL_HEADER.MajorLinkerVersion,
            pe.OPTIONAL_HEADER.MinorLinkerVersion,
            pe.OPTIONAL_HEADER.SizeOfCode,
            pe.OPTIONAL_HEADER.SizeOfInitializedData,
            pe.OPTIONAL_HEADER.SizeOfUninitializedData,
            pe.OPTIONAL_HEADER.AddressOfEntryPoint,
            pe.OPTIONAL_HEADER.BaseOfCode,
            pe.OPTIONAL_HEADER.ImageBase,
            pe.OPTIONAL_HEADER.SectionAlignment,
            pe.OPTIONAL_HEADER.FileAlignment,
            pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MajorImageVersion,
            pe.OPTIONAL_HEADER.MinorImageVersion,
            pe.OPTIONAL_HEADER.MajorSubsystemVersion,
            pe.OPTIONAL_HEADER.MinorSubsystemVersion,
            pe.OPTIONAL_HEADER.SizeOfImage,
            pe.OPTIONAL_HEADER.SizeOfHeaders,
            pe.OPTIONAL_HEADER.CheckSum,
            pe.OPTIONAL_HEADER.Subsystem,
            pe.OPTIONAL_HEADER.DllCharacteristics,
            pe.OPTIONAL_HEADER.SizeOfStackReserve,
            pe.OPTIONAL_HEADER.SizeOfStackCommit,
            pe.OPTIONAL_HEADER.SizeOfHeapReserve,
            pe.OPTIONAL_HEADER.SizeOfHeapCommit,
            pe.OPTIONAL_HEADER.LoaderFlags,
            pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,
            len(pe.sections),  # Number of sections
        ]
        
        # Section features
        sections_mean_entropy = np.mean([section.get_entropy() for section in pe.sections])
        sections_min_entropy = np.min([section.get_entropy() for section in pe.sections])
        sections_max_entropy = np.max([section.get_entropy() for section in pe.sections])
        sections_mean_rawsize = np.mean([section.SizeOfRawData for section in pe.sections])
        sections_min_rawsize = np.min([section.SizeOfRawData for section in pe.sections])
        sections_max_rawsize = np.max([section.SizeOfRawData for section in pe.sections])
        sections_mean_virtualsize = np.mean([section.Misc_VirtualSize for section in pe.sections])
        sections_min_virtualsize = np.min([section.Misc_VirtualSize for section in pe.sections])
        sections_max_virtualsize = np.max([section.Misc_VirtualSize for section in pe.sections])
        
        features.extend([
            sections_mean_entropy,
            sections_min_entropy,
            sections_max_entropy,
            sections_mean_rawsize,
            sections_min_rawsize,
            sections_max_rawsize,
            sections_mean_virtualsize,
            sections_min_virtualsize,
            sections_max_virtualsize,
        ])
        
        # Import features
        imports_nb_dll = len(pe.DIRECTORY_ENTRY_IMPORT) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb = sum([len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb_ordinal = sum([len([imp for imp in entry.imports if imp.ordinal]) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        
        features.extend([
            imports_nb_dll,
            imports_nb,
            imports_nb_ordinal,
        ])
        
        # Export features
        export_nb = len(pe.DIRECTORY_ENTRY_EXPORT.symbols) if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') else 0
        features.append(export_nb)
        
        # Resource features
        resources_nb = len(pe.DIRECTORY_ENTRY_RESOURCE.entries) if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0
        resources = []
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(entry, 'directory'):
                    for res in entry.directory.entries:
                        data_rva = res.data.struct.OffsetToData
                        size = res.data.struct.Size
                        data = pe.get_data(data_rva, size)
                        entropy = entropy_calculate(data)
                        resources.append((entropy, size))
        
        if resources:
            resources_mean_entropy = np.mean([res[0] for res in resources])
            resources_min_entropy = np.min([res[0] for res in resources])
            resources_max_entropy = np.max([res[0] for res in resources])
            resources_mean_size = np.mean([res[1] for res in resources])
            resources_min_size = np.min([res[1] for res in resources])
            resources_max_size = np.max([res[1] for res in resources])
        else:
            resources_mean_entropy = 0
            resources_min_entropy = 0
            resources_max_entropy = 0
            resources_mean_size = 0
            resources_min_size = 0
            resources_max_size = 0
        
        features.extend([
            resources_nb,
            resources_mean_entropy,
            resources_min_entropy,
            resources_max_entropy,
            resources_mean_size,
            resources_min_size,
            resources_max_size,
        ])
        
        # Load configuration size
        load_configuration_size = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size if hasattr(pe, 'DIRECTORY_ENTRY_LOAD_CONFIG') else 0
        features.append(load_configuration_size)
        
        # Version information size
        version_information_size = len(pe.VS_FIXEDFILEINFO) if hasattr(pe, 'VS_FIXEDFILEINFO') else 0
        features.append(version_information_size)
        
        return features
    
    except Exception as e:
        print(f"Error extracting features: {e}")
        return [0] * 50  # Return a default feature vector in case of error

def entropy_calculate(data):
    """Calculate the entropy of a given data."""
    if not data:
        return 0
    entropy = 0
    data_len = len(data)
    freq = {}
    for byte in data:
        if byte in freq:
            freq[byte] += 1
        else:
            freq[byte] = 1
    
    for byte in freq:
        p_x = freq[byte] / data_len
        entropy -= p_x * np.log2(p_x)
    return entropy

# def compress_file(file_path):
#     # Implement file compression logic here
#     # Return the path to the compressed file
#     compressed_file_path = file_path  # Placeholder
#     return compressed_file_path




def Home(request):
    if request.method == 'POST' and request.FILES['file']:
        uploaded_file = request.FILES['file']
        fs = FileSystemStorage()
        file_path = fs.save(uploaded_file.name, uploaded_file)
        file_url = fs.url(file_path)

        # Read the uploaded file and extract features
        file_features = extract_features(file_path)

        # Predict using the loaded model
        is_malware = model.predict([file_features])[0]

        if is_malware == 1:
            return JsonResponse({'message': 'The file is detected as malware.', 'malware': True})
        else:
            # compressed_file_path = compress_file(file_path)
            compressed_file_path = "Compress SUccssfully"
            return JsonResponse({'message': 'The file is safe and has been compressed.', 'malware': False, 'compressed_file_url': compressed_file_path})

    return render(request, 'home.html')