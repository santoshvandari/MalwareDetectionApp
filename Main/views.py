import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import pickle
import pefile
from django.shortcuts import render
from django.http import JsonResponse
from django.core.files.storage import FileSystemStorage
import os
from core.settings import BASE_DIR

# Load the trained model
model_path = os.path.join(BASE_DIR, 'MalwareModel/Malware.pkl')
with open(model_path, 'rb') as model_file:
    model = pickle.load(model_file)

def extract_features(file_path):
    try:
        pe = pefile.PE(file_path)
        
        features = [
            pe.FILE_HEADER.Machine,
            pe.FILE_HEADER.SizeOfOptionalHeader,
            pe.FILE_HEADER.Characteristics,
            pe.OPTIONAL_HEADER.MajorLinkerVersion,
            pe.OPTIONAL_HEADER.MinorLinkerVersion,
            pe.OPTIONAL_HEADER.SizeOfCode,
            pe.OPTIONAL_HEADER.SizeOfInitializedData,
            pe.OPTIONAL_HEADER.SizeOfUninitializedData,
            pe.OPTIONAL_HEADER.AddressOfEntryPoint,
            pe.OPTIONAL_HEADER.BaseOfCode,
            pe.OPTIONAL_HEADER.ImageBase,
            pe.OPTIONAL_HEADER.SectionAlignment,
            pe.OPTIONAL_HEADER.FileAlignment,
            pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
            pe.OPTIONAL_HEADER.MajorImageVersion,
            pe.OPTIONAL_HEADER.MinorImageVersion,
            pe.OPTIONAL_HEADER.MajorSubsystemVersion,
            pe.OPTIONAL_HEADER.MinorSubsystemVersion,
            pe.OPTIONAL_HEADER.SizeOfImage,
            pe.OPTIONAL_HEADER.SizeOfHeaders,
            pe.OPTIONAL_HEADER.CheckSum,
            pe.OPTIONAL_HEADER.Subsystem,
            pe.OPTIONAL_HEADER.DllCharacteristics,
            pe.OPTIONAL_HEADER.SizeOfStackReserve,
            pe.OPTIONAL_HEADER.SizeOfStackCommit,
            pe.OPTIONAL_HEADER.SizeOfHeapReserve,
            pe.OPTIONAL_HEADER.SizeOfHeapCommit,
            pe.OPTIONAL_HEADER.LoaderFlags,
            pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,
            len(pe.sections),  # Number of sections
        ]
        
        # Section features
        sections_entropy = [section.get_entropy() for section in pe.sections]
        features.extend([
            np.mean(sections_entropy),
            np.min(sections_entropy),
            np.max(sections_entropy),
            np.mean([section.SizeOfRawData for section in pe.sections]),
            np.min([section.SizeOfRawData for section in pe.sections]),
            np.max([section.SizeOfRawData for section in pe.sections]),
            np.mean([section.Misc_VirtualSize for section in pe.sections]),
            np.min([section.Misc_VirtualSize for section in pe.sections]),
            np.max([section.Misc_VirtualSize for section in pe.sections]),
        ])
        
        # Import features
        imports_nb_dll = len(pe.DIRECTORY_ENTRY_IMPORT) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb = sum([len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        imports_nb_ordinal = sum([len([imp for imp in entry.imports if imp.ordinal]) for entry in pe.DIRECTORY_ENTRY_IMPORT]) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        features.extend([imports_nb_dll, imports_nb, imports_nb_ordinal])
        
        # Export features
        export_nb = len(pe.DIRECTORY_ENTRY_EXPORT.symbols) if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') else 0
        features.append(export_nb)
        
        # Resource features
        resources = []
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(entry, 'directory'):
                    for res in entry.directory.entries:
                        data_rva = res.data.struct.OffsetToData
                        size = res.data.struct.Size
                        data = pe.get_data(data_rva, size)
                        entropy = entropy_calculate(data)
                        resources.append((entropy, size))
        
        if resources:
            resources_mean_entropy = np.mean([res[0] for res in resources])
            resources_min_entropy = np.min([res[0] for res in resources])
            resources_max_entropy = np.max([res[0] for res in resources])
            resources_mean_size = np.mean([res[1] for res in resources])
            resources_min_size = np.min([res[1] for res in resources])
            resources_max_size = np.max([res[1] for res in resources])
        else:
            resources_mean_entropy = resources_min_entropy = resources_max_entropy = 0
            resources_mean_size = resources_min_size = resources_max_size = 0
        
        features.extend([
            len(pe.DIRECTORY_ENTRY_RESOURCE.entries) if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0,
            resources_mean_entropy,
            resources_min_entropy,
            resources_max_entropy,
            resources_mean_size,
            resources_min_size,
            resources_max_size,
        ])
        
        # Load configuration size
        load_configuration_size = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size if hasattr(pe, 'DIRECTORY_ENTRY_LOAD_CONFIG') else 0
        features.append(load_configuration_size)
        
        # Version information size
        version_information_size = len(pe.VS_FIXEDFILEINFO) if hasattr(pe, 'VS_FIXEDFILEINFO') else 0
        features.append(version_information_size)
        
        return features
    
    except Exception as e:
        print(f"Error extracting features: {e}")
        return [0] * 53  # Ensure the feature vector matches the training data

def entropy_calculate(data):
    """Calculate the entropy of a given data."""
    if not data:
        return 0
    entropy = 0
    data_len = len(data)
    freq = {}
    for byte in data:
        if byte in freq:
            freq[byte] += 1
        else:
            freq[byte] = 1
    
    for byte in freq:
        p_x = freq[byte] / data_len
        entropy -= p_x * np.log2(p_x)
    return entropy

def Home(request):
    if request.method == 'POST' and request.FILES['file']:
        uploaded_file = request.FILES['file']
        fs = FileSystemStorage()
        file_path = fs.save(uploaded_file.name, uploaded_file)
        file_path = os.path.join(BASE_DIR, fs.url(file_path).lstrip('/'))
        print(file_path)
        # Check if the file exists
        if not os.path.exists(file_path):
            return JsonResponse({'message': 'File not found.', 'malware': False})

        # Read the uploaded file and extract features
        file_features = extract_features(file_path)

        # Predict using the loaded model
        is_malware = model.predict([file_features])[0]

        if is_malware == 1:
            return JsonResponse({'message': 'The file is detected as malware.', 'malware': True})
        else:
            # compressed_file_path = compress_file(file_path)
            compressed_file_path = "Compress Successfully"
            return JsonResponse({'message': 'The file is safe and has been compressed.', 'malware': False, 'compressed_file_url': compressed_file_path})

    return render(request, 'index.html')

# /home/wabisabi/Desktop/MalwareDetectionApp/media/error.exe

